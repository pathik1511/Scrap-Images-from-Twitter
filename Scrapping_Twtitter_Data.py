# -*- coding: utf-8 -*-
"""twitter_Api_V2_updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mjl51AB3sSg3dtDWZVRsE-wS7a3zHqc6
"""

import requests
# For saving access tokens and for file management when creating and adding to the dataset
import os
# For dealing with json responses we receive from the API
import json
# For displaying the data after
import pandas as pd
# For saving the response data in CSV format
import csv
# For parsing the dates received from twitter in readable formats
import datetime
import dateutil.parser
import unicodedata
#To add wait time between requests
import time
import pandas as pd
import numpy as np

os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAFllbgEAAAAAcoG6TOcqMFuy%2BzOlW7vOt1DICek%3DRBuE6Pj4cxoFl7RL96EZpFJuZjQnsHab5dhC5DVYbNysKaaFJL'

def auth():
    return os.getenv('TOKEN')

def create_headers(bearer_token):
    headers = {"Authorization": "Bearer {}".format(bearer_token)}
    return headers

def create_url(keyword, start_date, end_date, max_results = 3000):
    
    search_url = "https://api.twitter.com/2/tweets/search/all" #Change to the endpoint you want to collect data from

    #change params based on the endpoint you are using
    query_params = {'query': '#NFL has:media',
                    'start_time': start_date,
                    'end_time': end_date,
                    'max_results': max_results,
                    # 'expansions': 'author_id,in_reply_to_user_id,geo.place_id',
                    'tweet.fields': 'id,text,created_at',
                    'expansions': 'attachments.media_keys,author_id',
                    'media.fields' : 'url',
                    # 'user.fields': 'id,username,created_at',
                    # 'place.fields': 'full_name,id,country,country_code,geo,name,place_type',
                    'next_token': {}}
    return (search_url, query_params)

def connect_to_endpoint(url, headers, params, next_token = None):
    params['next_token'] = next_token   #params object received from create_url function
    response = requests.request("GET", url, headers = headers, params = params)
    print("Endpoint Response Code: " + str(response.status_code))
    if response.status_code != 200:
        raise Exception(response.status_code, response.text)
    return response.json()

# bearer_token = auth()
# headers = create_headers(bearer_token)
# keyword = "#NFL has:media"
# start_time = "2020-10-19T00:00:00.000Z"
# end_time = "2021-10-31T00:00:00.000Z"
# max_results = 100

# url = create_url(keyword,start_time,end_time, max_results)
# print(url)

# json_response = connect_to_endpoint(url[0], headers, url[1])

# print(json.dumps(json_response, indent=10, sort_keys=True))

# a=[]
# #df = pd.DataFrame(np.random.rand(100000,1),columns=['url'])
# for tweet in json_response['includes']['users']:
#   if 'id' in tweet.keys():
#     # a.append(tweet.get('url'))
#     a.append(tweet.get('id'))

# b=[]
# df = pd.DataFrame(np.random.rand(100000,1),columns=['url'])
# for tweet in json_response['includes']['media']:
#   if 'url' in tweet.keys():
#     # a.append(tweet.get('url'))
#     b.append(tweet.get('url'))

def append_to_csv(json_response, fileName,img):

    #A counter variable
    counter = 0
    

    #Open OR create the target CSV file
    csvFile = open(fileName, "a", newline="", encoding='utf-8')
    csvWriter = csv.writer(csvFile)

    #Loop through each tweet
    for tweet in json_response['data']:
        date = dateutil.parser.parse(tweet['created_at'])
        tid = tweet['id']
        text = tweet['text']
        # if ('entities.urls.images' in tweet):
        #   url = tweet['entities']['urls']['images']
        # else:
        #   url = "None"
        
        # Assemble all data in a list
        res = [date ,tid,  text]
        # Append the result to the CSV file
        csvWriter.writerow(res)
        counter += 1
    
    for tweet in json_response['includes']['media']:
      if 'url' in tweet.keys():
        img.append(tweet.get('url'))
    
    print("Total Urls = {}".format(len(img)))
    dict = {'Url':img}
    df = pd.DataFrame(dict)
    df.to_csv('img.csv')

    
    # counter=0
    # csvFile = open("urls.csv", "a", newline="", encoding='utf-8')
    # csvWriter = csv.writer(csvFile)
    # img=[]
    
    # for tweet in json_response['includes']['media']:
    #   if 'url' in tweet.keys():
    #     img.append(tweet.get('url'))
        # dict = {'Url':res}
    # for i,tweet in enumerate(json_response['data']):
      
    #   if ('tweet.entities.urls.images.url' in tweet):
    #     url = tweet['entities']['urls']['images'][0]['url']
        
      # else:
      #   url = "None"
      
      # csvWriter.writerow(res)
      # counter += 1

    # When done, close the CSV file
    csvFile.close()
    # Print the number of tweets for this iteration
    print("# of Tweets added from this response: ", counter)



"""	Feb. 13, 2022"""

#Inputs for tweets
bearer_token = auth()
headers = create_headers(bearer_token)
keyword = "#NFL has:media"
start_list =    ['2022-03-1T00:00:00.000Z']

end_list =      ['2022-03-07T00:00:00.000Z']
# max_results = 500
# start_time = "2020-10-19T00:00:00.000Z"
# end_time = "2021-10-31T00:00:00.000Z"
max_results = 500

#Total number of tweets we collected from the loop
total_tweets = 0

# Create file
csvFile = open("data.csv", "a", newline="", encoding='utf-8')
csvWriter = csv.writer(csvFile)

#Create headers for the data you want to save, in this example, we only want save these columns in our dataset
csvWriter.writerow([ 'date','tid','tweet'])
csvFile.close()
img=[]

for i in range(0,len(start_list)):

    # Inputs
    count = 0 # Counting tweets per time period
    max_count = 100000 # Max tweets per time period
    flag = True
    next_token = None
    
    # Check if flag is true
    while flag:
        # Check if max_count reached
        if count >= max_count:
            break
        print("-------------------")
        print("Token: ", next_token)
        url = create_url(keyword, start_list[i],end_list[i], max_results)
        json_response = connect_to_endpoint(url[0], headers, url[1], next_token)
        result_count = json_response['meta']['result_count']

        if 'next_token' in json_response['meta']:
            # Save the token to use for next call
            next_token = json_response['meta']['next_token']
            print("Next Token: ", next_token)
            if result_count is not None and result_count > 0 and next_token is not None:
                print("Start Date: ", start_list[i])
                append_to_csv(json_response, "data.csv",img)
                count += result_count
                total_tweets += result_count
                print("Total # of Tweets added: ", total_tweets)
                print("-------------------")
                time.sleep(5)                
        # If no next token exists
        else:
            if result_count is not None and result_count > 0:
                print("-------------------")
                print("Start Date: ", start_list[i])
                append_to_csv(json_response, "data.csv")
                count += result_count
                total_tweets += result_count
                print("Total # of Tweets added: ", total_tweets)
                print("-------------------")
                time.sleep(5)
            
            #Since this is the final request, turn flag to false to move to the next time period.
            flag = False
            next_token = None
        time.sleep(5)
print("Total number of results: ", total_tweets)

"""#for downloading Images from urls"""

from re import U
import sys
import urllib.request

def url_to_jpg(i,url,FilePath):
  
  filename = 'image-{}.jpg'.format(i)
  full_path = '{}{}'.format(FilePath,filename)
  urllib.request.urlretrieve(url,full_path)
  print('{} saved.'.format(filename))

  
  return None

Filename = 'img.csv'
FilePath = '/content/Images/'
urls = pd.read_csv(Filename)


for i,url in enumerate(urls.values):
  url_to_jpg(i,url[1],FilePath)

!zip -r /content/after_Game_Week3_Images.zip /content/Images/

from google.colab import files
files.download("after_Game_Week3_Images.zip")

!cp after_Game_Week1_Images.zip "/content/drive/MyDrive"

from google.colab import drive
drive.mount('/content/drive')

